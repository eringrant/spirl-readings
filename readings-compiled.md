---
bibliography: 'all.bib'
csl: 'chicago-syllabus.csl'
---

### Related Work

*Readings list currently under construction. Check back soon for more!*

For reference, we have compiled a list of some important publications on
the topic of structure and priors in reinforcement learning. Please make
a pull request at the [spirl-readings
repository](https://github.com/eringrant/spirl-readings) or email us at
<organizers@spirl.info> if there's relevant work that could be added to
the list!

#### Reinforcement Learning

-   Peter Dayan and Geoffrey E. Hinton, “Feudal Reinforcement Learning,”
    in *NeurIPS*, 1992,
    <http://papers.nips.cc/paper/714-feudal-reinforcement-learning>.
-   Satinder P. Singh, “Transfer of Learning by Composing Solutions of
    Elemental Sequential Tasks” 8 (1992): 323–339,
    <https://doi.org/10.1007/BF00992700>.
-   Peter Dayan and Geoffrey E Hinton, “Feudal Reinforcement Learning,”
    in *Advances in Neural Information Processing Systems*, 1993,
    271–278.
-   Sebastian Thrun and Anton Schwartz, “Finding Structure in
    Reinforcement Learning,” in *Advances in Neural Information
    Processing Systems*, 1995, 385–392.
-   Ronald Parr and Stuart J Russell, “Reinforcement Learning with
    Hierarchies of Machines,” in *NeurIPS*, 1998, 1043–1049.
-   Richard S Sutton, Doina Precup, and Satinder Singh, “Between MDPs
    and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement
    Learning” 112, nos. 1-2 (1999): 181–211.
-   Thomas G Dietterich, “Hierarchical Reinforcement Learning with the
    MAXQ Value Function Decomposition” 13 (2000): 227–303.
-   Michael L. Littman, Richard S. Sutton, and Satinder P. Singh,
    “Predictive Representations of State,” in *NeurIPS*, 2001.
-   Kfir Y Levy and Nahum Shimkin, “Unified Inter and Intra Options
    Learning Using Policy Gradient Methods,” in *European Workshop on
    Reinforcement Learning* (Springer, 2011), 153–164.
-   Carlos Diuk et al., “Divide and Conquer: Hierarchical Reinforcement
    Learning and Task Decomposition in Humans,” in *Computational and
    Robotic Models of the Hierarchical Organization of Behavior*
    (Springer, 2013), 271–291.

#### Cognitive Science

-   Rachit Dubey et al., “Investigating Human Priors for Playing Video
    Games,” in *ICML*, 2018.

Dayan, Peter, and Geoffrey E Hinton. “Feudal Reinforcement Learning.” In
*Advances in Neural Information Processing Systems*, 271–278, 1993.

Dayan, Peter, and Geoffrey E. Hinton. “Feudal Reinforcement Learning.”
In *NeurIPS*, 1992.
<http://papers.nips.cc/paper/714-feudal-reinforcement-learning>.

Dietterich, Thomas G. “Hierarchical Reinforcement Learning with the MAXQ
Value Function Decomposition” 13 (2000): 227–303.

Diuk, Carlos, Anna Schapiro, Natalia Córdova, José Ribas-Fernandes, Yael
Niv, and Matthew Botvinick. “Divide and Conquer: Hierarchical
Reinforcement Learning and Task Decomposition in Humans.” In
*Computational and Robotic Models of the Hierarchical Organization of
Behavior*, 271–291. Springer, 2013.

Dubey, Rachit, Pulkit Agrawal, Deepak Pathak, Thomas L Griffiths, and
Alexei A Efros. “Investigating Human Priors for Playing Video Games.” In
*ICML*, 2018.

Levy, Kfir Y, and Nahum Shimkin. “Unified Inter and Intra Options
Learning Using Policy Gradient Methods.” In *European Workshop on
Reinforcement Learning*, 153–164. Springer, 2011.

Littman, Michael L., Richard S. Sutton, and Satinder P. Singh.
“Predictive Representations of State.” In *NeurIPS*, 2001.

Parr, Ronald, and Stuart J Russell. “Reinforcement Learning with
Hierarchies of Machines.” In *NeurIPS*, 1043–1049, 1998.

Singh, Satinder P. “Transfer of Learning by Composing Solutions of
Elemental Sequential Tasks” 8 (1992): 323–339.
<https://doi.org/10.1007/BF00992700>.

Sutton, Richard S, Doina Precup, and Satinder Singh. “Between MDPs and
Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement
Learning” 112, nos. 1-2 (1999): 181–211.

Thrun, Sebastian, and Anton Schwartz. “Finding Structure in
Reinforcement Learning.” In *Advances in Neural Information Processing
Systems*, 385–392, 1995.
